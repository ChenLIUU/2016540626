# 第一步要对网页进行访问
from urllib import request
from bs4 import BeautifulSoup as bs
import re
import jieba  # 分词包
import pandas as pd
import numpy
import matplotlib

matplotlib.rcParams['figure.figsize'] = (10.0, 5.0)
from matplotlib import pyplot as plt
import matplotlib
from wordcloud import WordCloud  # 词云包
from scipy.misc import imread

resp = request.urlopen('https://movie.douban.com/cinema/nowplaying/changsha/ ')
html_data = resp.read().decode('utf-8')
# print(html_data)
# 第二步，需要对得到的html代码进行解析，得到里面提取我们需要的数据
soup = bs(html_data, 'html.parser')
# div id=”nowplaying“标签开始是我们想要的数据，里面有电影的名称、评分、主演等信息
nowplaying_movie = soup.find_all('div', id="nowplaying")
nowplaying_movie_list = nowplaying_movie[0].find_all('li', class_='list-item')
# data-subject属性里面放了电影的id号码，而在img标签的alt属性里面放了电影的名字，因此我们就通过这两个属性来得到电影的id和名称
# print(nowplaying_movie_list)
nowplaying_list = []
for item in nowplaying_movie_list:
    nowplaying_dict = {}
nowplaying_dict['id'] = item['data-subject']
for tag_img_item in item.find_all('img'):
    nowplaying_dict['name'] = tag_img_item['alt']
nowplaying_list.append(nowplaying_dict)
print(nowplaying_dict['name'])
print(nowplaying_list[3]['id'])

""""《毒液》的短评网址为：https://movie.douban.com/subject/3168101/comments?start=0&limit=20


其中3168101就是电影的id，start=0表示评论的第0条评论。接下来接对该网址进行解析了。///"""
for movie in range(5):
    requrl = 'https://movie.douban.com/subject/

             ' + nowplaying_list[movie]['
id
'] + ' / comments
' +'?' +'
start = 0
' + ' & limit = 1000
'
resp = request.urlopen(requrl)
html_data = resp.read().decode('utf-8')
soup = bs(html_data, 'html.parser')
comment_div_lits = soup.find_all('span', class_='short')
"""此时在comment_div_lits 列表中存放的就是div标签和comment属性下面的html代码了。在上图中还可以发现在p标签下面存放了网友对电影的评论，"""
# 对comment_div_lits 代码中的html代码继续进行解析，代码如下：
print(comment_div_lits)
'''eachCommentList = []#这是我掉入的坑，费了好大得劲才爬出来，根本没必要那么麻烦，直接找影评标签去提取就好，这里坑了我很久很久
for item in comment_div_lits:
        if item.find_all('span')[0].string is not None:
            eachCommentList.append(item.find_all('span')[0].string)
print(eachCommentList)'''

"""二、数据清洗

为了方便进行数据进行清洗，我们将列表中的数据放在一个字符串数组中，"""
comments = ''
for k in range(len(comment_div_lits)):
    comments = comments + (str(comment_div_lits[k])).strip()
print(comments)
"""可以看到所有的评论已经变成一个字符串了，但是我们发现评论中还有不少的标点符号等。这些符号对我们进行词频统计时根本没有用，因此要将它们清除。所用的方法是正则表达式。
python中正则表达式是通过re模块来实现的。"""

pattern = re.compile(r'[\u4e00-\u9fa5]+')
filterdata = re.findall(pattern, comments)
cleaned_comments = ''.join(filterdata)
print(cleaned_comments)
"""此时评论数据中已经没有那些标点符号了，数据变得“干净”了很多。
因此要进行词频统计，所以先要进行中文分词操作。在这里我使用的是结巴分词。
如果没有安装结巴分词，可以在控制台使用pip install jieba进行安装。（注：可以使用pip list查看是否安装了这些库）。"""

segment = jieba.lcut(cleaned_comments)
words_df = pd.DataFrame({'segment': segment})
print(words_df)
"""我们的数据中有“看”、“太”、“的”等虚词（停用词），而这些词在任何场景中都是高频时，并且没有实际的含义，所以我们要他们进行清除。

我把停用词放在一个stopwords.txt文件中，将我们的数据与停用词进行比对即可（注：只要在百度中输入stopwords.txt，就可以下载到该文件）。停用词已经被出去了"""
stopwords = pd.read_csv("D:\PyCharm 2018.2.3\chineseStopWords.txt", index_col=False, quoting=3, sep="\t", names=['stopword'], encoding='utf-8')
# quoting=3全不引用
words_df = words_df[~words_df.segment.isin(stopwords.stopword)]
print(words_df.head())

# 利用numpy计算包进行词频统计
words_stat = words_df.groupby(by=['segment'])['segment'].agg({"计数": numpy.size})
words_stat = words_stat.reset_index().sort_values(by=["计数"], ascending=False)
print(words_stat.head())

#image = imread("3.jpg")
matplotlib.rcParams['figure.figsize'] = (10.0, 5.0)
wordcloud = WordCloud(font_path="simhei.ttf", background_color="white", max_font_size=80, mask=image)
word_frequence = {x[0]: x[1] for x in words_stat.head(1000).values}
wordcloud = wordcloud.fit_words(word_frequence)
plt.imshow(wordcloud)
plt.show()
